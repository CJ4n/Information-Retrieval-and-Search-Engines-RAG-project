{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5hH-WBI1R2H"
      },
      "source": [
        "# H02C8b Information Retrieval and Search Engines: RAG Project\n",
        "\n",
        "Welcome to the notebook companion for the IRSE project. You will find all starter code here. You are encouraged to use this code, as it has been confirmed to work for the RAG pipeline described in the assignment handout. However, you are certainly welcome to make any changes you see fit, provided that your code is written in Python and runs without issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kkz_9Y7FrLe"
      },
      "source": [
        "**IMPORTANT**: Do not submit a notebook as your final solution. It will not be graded. Refer to assignment handout for more information about the submission format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2_H111Frbu"
      },
      "source": [
        "**IMPORTANT**: Be mindful of your runtime usage, if working in Colab. At the beginning of every session, navigate to the top menu bar in Colab and select **Runtime > Change runtime type > CPU (Python 3)**. This will ensure that your session runs on CPU and that you do not waste any GPU allocation for the day. GPUs are provided by Google on a limited daily basis, and access is given every 24 hours. It is best that you complete the TF-IDF/search component before loading models and running inference on the GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9AFxoyfQZT"
      },
      "source": [
        "If you have any questions, feel free to email [Thomas](mailto:thomas.bauwens@kuleuven.be) or [Kushal](mailto:kushaljayesh.tatariya@kuleuven.be)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpcHebZfGi-f"
      },
      "source": [
        "## RAG for recipe recommendation:\n",
        "\n",
        "We will begin by installing the huggingface `datasets` library for easily loading our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oy3FsTE2bKfU"
      },
      "outputs": [],
      "source": [
        "# ! pip -q install datasets\n",
        "# !wget https://people.cs.kuleuven.be/~thomas.bauwens/irse_documents_2025_recipes.parquet\n",
        "# !wget https://people.cs.kuleuven.be/~thomas.bauwens/irse_queries_2025_recipes.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uIq7VwBoFvV1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yan/yan_files/2_semester/information_retrieval/project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to /home/yan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/yan/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/yan/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/yan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/yan/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/yan/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/yan/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import datasets\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import datasets\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()  # Show progress bar if using pandas\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# from google.colab import userdata\n",
        "# userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "08BidyMMGSpB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 231637\n",
            "Number of queries: 47\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"parquet\", data_files=\"./irse_documents_2025_recipes.parquet\"\n",
        ")[\"train\"]\n",
        "queries_data = json.load(open(\"./irse_queries_2025_recipes.json\", \"r\"))\n",
        "\n",
        "df = dataset.to_pandas()\n",
        "\n",
        "# Now you can apply the function to concatenate columns\n",
        "recipies = df.apply(\n",
        "    lambda row: f\"{row['name']} {row['description']} {row['ingredients']} {row['steps']}\", axis=1\n",
        ")#[:10000]\n",
        "recipe_ids = dataset[\"official_id\"]#[:10000]\n",
        "print(\"Number of documents:\", len(recipies))\n",
        "\n",
        "queries = pd.DataFrame(columns=[\"q\", \"r\", \"a\"])\n",
        "for query_item in queries_data[\"queries\"]:\n",
        "    query_text = query_item[\"q\"]\n",
        "    relevance_pairs = query_item[\"r\"]\n",
        "    answer = query_item[\"a\"]\n",
        "    queries = pd.concat(\n",
        "        [\n",
        "            queries,\n",
        "            pd.DataFrame({\"q\": [query_text], \"r\": [relevance_pairs], \"a\": [answer]}),\n",
        "        ],\n",
        "        ignore_index=True,\n",
        "    )\n",
        "\n",
        "print(\"Number of queries:\", len(queries))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/231637 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_fast(doc):\n",
        "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
        "\n",
        "    words = word_tokenize(doc)\n",
        "\n",
        "    words = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in words\n",
        "        if word not in stop_words #and word.isalpha()\n",
        "    ]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "preprocessed_recipes = [preprocess_fast(doc) for doc in tqdm(recipies)]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PEQStiG7xtC"
      },
      "source": [
        "#### Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAEZMxZGus6M"
      },
      "outputs": [],
      "source": [
        "# lemmatizer = WordNetLemmatizer()\n",
        "# counter = 0\n",
        "\n",
        "\n",
        "# def preprocess(doc):\n",
        "#     global counter\n",
        "#     counter += 1\n",
        "#     if counter % 1000 == 0:\n",
        "#         print(f\"Processed: {counter}\")\n",
        "#     doc = doc.split()\n",
        "#     preprocessed_text = []\n",
        "#     for text in doc:\n",
        "#         text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "#         text = text.lower()\n",
        "#         words = word_tokenize(text)\n",
        "#         words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "#         words = [lemmatizer.lemmatize(word) for word in words]\n",
        "#         if words != []:\n",
        "#             preprocessed_text.append(words[0])\n",
        "#     return preprocessed_text\n",
        "# vectorizer = TfidfVectorizer(tokenizer=preprocess)\n",
        "# X = vectorizer.fit_transform(recipies)\n",
        "# print(\"fitted\")\n",
        "# tfidf_df = pd.DataFrame(\n",
        "#     X.toarray(), index=range(len(recipies)), columns=vectorizer.get_feature_names_out()\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer()  \n",
        "X = vectorizer.fit_transform(preprocessed_recipes)\n",
        "print(\"TF-IDF fitted\")\n",
        "\n",
        "# # Optional: convert to DataFrame (can be memory-heavy)\n",
        "# tfidf_df = pd.DataFrame(\n",
        "#     X.toarray(), index=range(len(preprocessed_recipes)), columns=vectorizer.get_feature_names_out()\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O9GR23l7xtC"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def calculate_precision_recall_f1_optimized(relevant_doc_ids, retrieved_doc_ids):\n",
        "    # Convert to sets for faster operations\n",
        "    relevant_set = set(relevant_doc_ids)\n",
        "    retrieved_set = set(retrieved_doc_ids)\n",
        "\n",
        "    # Calculate true positives (documents that are both relevant and retrieved)\n",
        "    true_positives = len(relevant_set.intersection(retrieved_set))\n",
        "\n",
        "    # Calculate precision, recall, and F1\n",
        "    if len(retrieved_set) == 0:\n",
        "        precision = 0.0\n",
        "        recall = 0.0 if len(relevant_set) > 0 else 1.0\n",
        "        f1 = 0.0\n",
        "    elif len(relevant_set) == 0:\n",
        "        precision = 0.0\n",
        "        recall = 1.0\n",
        "        f1 = 0.0\n",
        "    else:\n",
        "        precision = true_positives / len(retrieved_set)\n",
        "        recall = true_positives / len(relevant_set)\n",
        "        if precision + recall > 0:\n",
        "            f1 = 2 * precision * recall / (precision + recall)\n",
        "        else:\n",
        "            f1 = 0.0\n",
        "\n",
        "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "\n",
        "def calculate_macro_averages(metrics_per_query):\n",
        "    # This function remains the same\n",
        "    precision_values = [metrics[\"precision\"] for metrics in metrics_per_query]\n",
        "    recall_values = [metrics[\"recall\"] for metrics in metrics_per_query]\n",
        "    f1_values = [metrics[\"f1\"] for metrics in metrics_per_query]\n",
        "\n",
        "    macro_precision = np.mean(precision_values)\n",
        "    macro_recall = np.mean(recall_values)\n",
        "    macro_f1 = np.mean(f1_values)\n",
        "\n",
        "    return {\n",
        "        \"macro_precision\": macro_precision,\n",
        "        \"macro_recall\": macro_recall,\n",
        "        \"macro_f1\": macro_f1,\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_micro_averages_optimized(all_relevant_doc_ids, all_retrieved_doc_ids):\n",
        "    # Flatten lists of relevant and retrieved document IDs\n",
        "    all_relevant = [\n",
        "        doc_id for query_relevant in all_relevant_doc_ids for doc_id in query_relevant\n",
        "    ]\n",
        "    all_retrieved = [\n",
        "        doc_id\n",
        "        for query_retrieved in all_retrieved_doc_ids\n",
        "        for doc_id in query_retrieved\n",
        "    ]\n",
        "\n",
        "    # Count true positives across all queries\n",
        "    relevant_set = set(all_relevant)\n",
        "    retrieved_set = set(all_retrieved)\n",
        "    true_positives = len(relevant_set.intersection(retrieved_set))\n",
        "\n",
        "    # Calculate micro-averaged metrics\n",
        "    if len(retrieved_set) == 0:\n",
        "        micro_precision = 0.0\n",
        "        micro_recall = 0.0 if len(relevant_set) > 0 else 1.0\n",
        "        micro_f1 = 0.0\n",
        "    elif len(relevant_set) == 0:\n",
        "        micro_precision = 0.0\n",
        "        micro_recall = 1.0\n",
        "        micro_f1 = 0.0\n",
        "    else:\n",
        "        micro_precision = true_positives / len(retrieved_set)\n",
        "        micro_recall = true_positives / len(relevant_set)\n",
        "        if micro_precision + micro_recall > 0:\n",
        "            micro_f1 = (\n",
        "                2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
        "            )\n",
        "        else:\n",
        "            micro_f1 = 0.0\n",
        "\n",
        "    return {\n",
        "        \"micro_precision\": micro_precision,\n",
        "        \"micro_recall\": micro_recall,\n",
        "        \"micro_f1\": micro_f1,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_documents(query_text, recipies, recipe_ids, k=None, threshold=None):\n",
        "    if len(recipies) != len(recipe_ids):\n",
        "        raise ValueError(\"Recipes and recipe_ids must have the same length\")\n",
        "\n",
        "    if not k and not threshold:\n",
        "        raise ValueError(\"Either k or threshold must be specified\")\n",
        "\n",
        "    if vectorizer is None or X is None:\n",
        "        raise ValueError(\"Vectorizer and document matrix X must be provided\")\n",
        "\n",
        "    query = preprocess_fast(query_text)\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    cosine_similarities = cosine_similarity(query_vector, X).flatten()\n",
        "\n",
        "    results = [\n",
        "        (recipies[i], recipe_ids[i], cosine_similarities[i])\n",
        "        for i in range(len(recipies))\n",
        "    ]\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    if threshold:\n",
        "        results = [r for r in results if r[2] >= threshold]\n",
        "\n",
        "    if k:\n",
        "        results = results[:k]\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_ir_system(queries, recipies, recipe_ids, k, threshold):\n",
        "    metrics_per_query = []\n",
        "    all_relevant_doc_ids = []\n",
        "    all_retrieved_doc_ids = []\n",
        "\n",
        "    for i, row in queries.iterrows():\n",
        "        query_text = row['q']\n",
        "        relevant_doc_ids = row['r']\n",
        "        relevant_doc_ids = [doc[0] for doc in relevant_doc_ids]\n",
        "\n",
        "        print(f\"\\nProcessing query {i + 1}/{len(queries)}: {query_text}\")\n",
        "\n",
        "        results = retrieve_documents(query_text, recipies, recipe_ids, k, threshold)\n",
        "        retrieved_doc_ids = [result[1] for result in results]\n",
        "\n",
        "        query_metrics = calculate_precision_recall_f1_optimized(\n",
        "            relevant_doc_ids, retrieved_doc_ids\n",
        "        )\n",
        "        metrics_per_query.append(query_metrics)\n",
        "\n",
        "        all_relevant_doc_ids.append(relevant_doc_ids)\n",
        "        all_retrieved_doc_ids.append(retrieved_doc_ids)\n",
        "\n",
        "        # print(\n",
        "        #     f\"Query {i + 1} metrics: Precision={query_metrics['precision']:.4f}, \"\n",
        "        #     f\"Recall={query_metrics['recall']:.4f}, F1={query_metrics['f1']:.4f}\"\n",
        "        # )\n",
        "\n",
        "    macro_metrics = calculate_macro_averages(metrics_per_query)\n",
        "    micro_metrics = calculate_micro_averages_optimized(\n",
        "        all_relevant_doc_ids, all_retrieved_doc_ids\n",
        "    )\n",
        "\n",
        "    all_metrics = {**macro_metrics, **micro_metrics}\n",
        "\n",
        "    return all_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "evaluate_ir_system(queries.iloc[[34]], recipies, recipe_ids,k=5, threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "metrics = evaluate_ir_system(queries, recipies, recipe_ids, k=5, threshold=None)\n",
        "\n",
        "print(\"\\n===== IR System Evaluation Results =====\")\n",
        "print(f\"Macro-average Precision: {metrics['macro_precision']:.4f}\")\n",
        "print(f\"Macro-average Recall: {metrics['macro_recall']:.4f}\")\n",
        "print(f\"Macro-average F1: {metrics['macro_f1']:.4f}\")\n",
        "print(f\"Micro-average Precision: {metrics['micro_precision']:.4f}\")\n",
        "print(f\"Micro-average Recall: {metrics['micro_recall']:.4f}\")\n",
        "print(f\"Micro-average F1: {metrics['micro_f1']:.4f}\")\n",
        "print(\"========================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def find_optimal_parameters(queries, recipies, recipe_ids):\n",
        "    \"\"\"\n",
        "    Find optimal threshold and k values for the retrieval system.\n",
        "    \n",
        "    Args:\n",
        "        queries: DataFrame containing queries and relevant document IDs\n",
        "        recipies: List of recipe documents\n",
        "        recipe_ids: List of recipe IDs\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with optimal parameters and results\n",
        "    \"\"\"\n",
        "    # Define parameter ranges\n",
        "    thresholds =  np.arange(0.1, 0.51, 0.05)  # 0.1 to 0.5 with 0.05 step\n",
        "    k_values = np.arange(1, 51, 2)  # 1 to 50 with step of 2\n",
        "    \n",
        "    # Results storage\n",
        "    results = []\n",
        "    \n",
        "    # Progress bar for the entire process\n",
        "    total_iterations = len(thresholds) + len(k_values)\n",
        "    progress_bar = tqdm(total=total_iterations, desc=\"Evaluating parameters\")\n",
        "    \n",
        "    # Evaluate fixed k values with varying thresholds\n",
        "    print(\"\\n\\n--- Testing different threshold values ---\")\n",
        "    for threshold in thresholds:\n",
        "        metrics = evaluate_ir_system(queries, recipies, recipe_ids, k=None, threshold=threshold)\n",
        "        results.append({\n",
        "            'parameter_type': 'threshold',\n",
        "            'value': threshold, \n",
        "            'k': None,\n",
        "            'threshold': threshold,\n",
        "            'macro_precision': metrics['macro_precision'],\n",
        "            'macro_recall': metrics['macro_recall'],\n",
        "            'macro_f1': metrics['macro_f1'],\n",
        "            'micro_precision': metrics['micro_precision'],\n",
        "            'micro_recall': metrics['micro_recall'],\n",
        "            'micro_f1': metrics['micro_f1']\n",
        "        })\n",
        "        \n",
        "        print(f\"Threshold={threshold:.2f}, Macro F1={metrics['macro_f1']:.4f}, Micro F1={metrics['micro_f1']:.4f}\")\n",
        "        progress_bar.update(1)\n",
        "    \n",
        "    # Evaluate fixed k values with varying thresholds\n",
        "    print(\"\\n\\n--- Testing different k values ---\")\n",
        "    for k in k_values:\n",
        "        metrics = evaluate_ir_system(queries, recipies, recipe_ids, k=int(k), threshold=None)\n",
        "        results.append({\n",
        "            'parameter_type': 'k',\n",
        "            'value': k,\n",
        "            'k': k,\n",
        "            'threshold': None,\n",
        "            'macro_precision': metrics['macro_precision'],\n",
        "            'macro_recall': metrics['macro_recall'],\n",
        "            'macro_f1': metrics['macro_f1'],\n",
        "            'micro_precision': metrics['micro_precision'],\n",
        "            'micro_recall': metrics['micro_recall'],\n",
        "            'micro_f1': metrics['micro_f1']\n",
        "        })\n",
        "        \n",
        "        print(f\"k={k}, Macro F1={metrics['macro_f1']:.4f}, Micro F1={metrics['micro_f1']:.4f}\")\n",
        "        progress_bar.update(1)\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    # Convert results to DataFrame for easier analysis\n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Find optimal threshold\n",
        "    threshold_results = results_df[results_df['parameter_type'] == 'threshold']\n",
        "    best_threshold_row = threshold_results.loc[threshold_results['macro_f1'].idxmax()]\n",
        "    best_threshold = best_threshold_row['threshold']\n",
        "    best_threshold_f1 = best_threshold_row['macro_f1']\n",
        "    \n",
        "    # Find optimal k\n",
        "    k_results = results_df[results_df['parameter_type'] == 'k']\n",
        "    best_k_row = k_results.loc[k_results['macro_f1'].idxmax()]\n",
        "    best_k = best_k_row['k']\n",
        "    best_k_f1 = best_k_row['macro_f1']\n",
        "    \n",
        "    # Determine overall best parameter\n",
        "    if best_threshold_f1 >= best_k_f1:\n",
        "        best_parameter = f\"threshold={best_threshold:.2f}\"\n",
        "        best_f1 = best_threshold_f1\n",
        "    else:\n",
        "        best_parameter = f\"k={int(best_k)}\"\n",
        "        best_f1 = best_k_f1\n",
        "    \n",
        "    print(f\"\\nBest threshold: {best_threshold:.2f} (F1: {best_threshold_f1:.4f})\")\n",
        "    print(f\"Best k: {int(best_k)} (F1: {best_k_f1:.4f})\")\n",
        "    print(f\"Overall best parameter: {best_parameter} (F1: {best_f1:.4f})\")\n",
        "    \n",
        "    return {\n",
        "        'results_df': results_df,\n",
        "        'best_threshold': best_threshold,\n",
        "        'best_k': int(best_k),\n",
        "        'best_parameter': best_parameter,\n",
        "        'best_f1': best_f1\n",
        "    }\n",
        "\n",
        "def plot_parameter_results(results_df):\n",
        "    \"\"\"\n",
        "    Plot the results of parameter tuning.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame containing evaluation results\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "     \n",
        "    # Plot 1: Threshold vs metrics\n",
        "    plt.subplot(2, 2, 1)\n",
        "    threshold_results = results_df[results_df['parameter_type'] == 'threshold']\n",
        "    \n",
        "    plt.plot(threshold_results['threshold'], threshold_results['macro_precision'], 'b-', label='Macro Precision')\n",
        "    plt.plot(threshold_results['threshold'], threshold_results['macro_recall'], 'r-', label='Macro Recall')\n",
        "    plt.plot(threshold_results['threshold'], threshold_results['macro_f1'], 'g-', label='Macro F1')\n",
        "    \n",
        "    best_threshold = threshold_results.loc[threshold_results['macro_f1'].idxmax()]['threshold']\n",
        "    plt.axvline(x=best_threshold, color='gray', linestyle='--', \n",
        "                label=f'Best Threshold={best_threshold:.2f}')\n",
        "    \n",
        "    plt.title('Threshold vs Macro Metrics')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: k vs metrics\n",
        "    plt.subplot(2, 2, 2)\n",
        "    k_results = results_df[results_df['parameter_type'] == 'k']\n",
        "    \n",
        "    plt.plot(k_results['k'], k_results['macro_precision'], 'b-', label='Macro Precision')\n",
        "    plt.plot(k_results['k'], k_results['macro_recall'], 'r-', label='Macro Recall')\n",
        "    plt.plot(k_results['k'], k_results['macro_f1'], 'g-', label='Macro F1')\n",
        "    \n",
        "    best_k = k_results.loc[k_results['macro_f1'].idxmax()]['k']\n",
        "    plt.axvline(x=best_k, color='gray', linestyle='--', \n",
        "                label=f'Best k={int(best_k)}')\n",
        "    \n",
        "    plt.title('k vs Macro Metrics')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Threshold vs micro metrics\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(threshold_results['threshold'], threshold_results['micro_precision'], 'b-', label='Micro Precision')\n",
        "    plt.plot(threshold_results['threshold'], threshold_results['micro_recall'], 'r-', label='Micro Recall')\n",
        "    plt.plot(threshold_results['threshold'], threshold_results['micro_f1'], 'g-', label='Micro F1')\n",
        "    \n",
        "    best_threshold_micro = threshold_results.loc[threshold_results['micro_f1'].idxmax()]['threshold']\n",
        "    plt.axvline(x=best_threshold_micro, color='gray', linestyle='--', \n",
        "                label=f'Best Threshold={best_threshold_micro:.2f}')\n",
        "    \n",
        "    plt.title('Threshold vs Micro Metrics')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: k vs micro metrics\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(k_results['k'], k_results['micro_precision'], 'b-', label='Micro Precision')\n",
        "    plt.plot(k_results['k'], k_results['micro_recall'], 'r-', label='Micro Recall')\n",
        "    plt.plot(k_results['k'], k_results['micro_f1'], 'g-', label='Micro F1')\n",
        "    \n",
        "    best_k_micro = k_results.loc[k_results['micro_f1'].idxmax()]['k']\n",
        "    plt.axvline(x=best_k_micro, color='gray', linestyle='--', \n",
        "                label=f'Best k={int(best_k_micro)}')\n",
        "    \n",
        "    plt.title('k vs Micro Metrics')\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('ir_parameter_tuning_results.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "    # Create heatmap for parameter combinations\n",
        "    if len(threshold_results) > 3 and len(k_results) > 3:\n",
        "        # We can create a separate heatmap for detailed analysis\n",
        "        create_combination_heatmap(results_df)\n",
        "\n",
        "def create_combination_heatmap(results_df):\n",
        "    \"\"\"\n",
        "    Create a heatmap for combinations of k and threshold values.\n",
        "    This requires running additional evaluations for all combinations.\n",
        "    \n",
        "    Args:\n",
        "        results_df: DataFrame with parameter evaluation results\n",
        "    \"\"\"\n",
        "    # Extract unique k and threshold values from previous runs\n",
        "    thresholds = sorted(results_df[results_df['parameter_type'] == 'threshold']['threshold'].unique())\n",
        "    k_values = sorted(results_df[results_df['parameter_type'] == 'k']['k'].unique())\n",
        "    \n",
        "    # Select a subset if there are too many values for a clear visualization\n",
        "    if len(thresholds) > 8:\n",
        "        thresholds = thresholds[::2]  # Take every other value\n",
        "    if len(k_values) > 8:\n",
        "        k_values = k_values[::2]  # Take every other value\n",
        "    \n",
        "    # Create a matrix to store F1 scores\n",
        "    f1_matrix = np.zeros((len(k_values), len(thresholds)))\n",
        "    \n",
        "    print(\"\\n\\n--- Testing combinations of k and threshold values ---\")\n",
        "    progress_bar = tqdm(total=len(k_values) * len(thresholds), desc=\"Evaluating combinations\")\n",
        "    \n",
        "    # Evaluate combinations\n",
        "    for i, k in enumerate(k_values):\n",
        "        for j, threshold in enumerate(thresholds):\n",
        "            # We need a special version that uses both k and threshold together\n",
        "            # Since we can't modify the retrieve_documents function, we'll use a trick:\n",
        "            # First get top k docs, then filter by threshold\n",
        "            metrics = evaluate_ir_system_combined(queries, recipies, recipe_ids, k=int(k), threshold=threshold)\n",
        "            f1_matrix[i, j] = metrics['macro_f1']\n",
        "            progress_bar.update(1)\n",
        "    \n",
        "    progress_bar.close()\n",
        "    \n",
        "    # Create heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(f1_matrix, annot=True, fmt=\".3f\", cmap=\"YlGnBu\", \n",
        "                xticklabels=[f\"{t:.2f}\" for t in thresholds], \n",
        "                yticklabels=[f\"{int(k)}\" for k in k_values])\n",
        "    \n",
        "    plt.title('Macro F1 Scores for Combinations of k and Threshold')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('k')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('ir_parameter_combination_heatmap.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_ir_system_combined(queries, recipies, recipe_ids, k, threshold):\n",
        "    \"\"\"\n",
        "    Evaluate IR system using both k and threshold together.\n",
        "    First retrieve top k docs, then filter by threshold.\n",
        "    \n",
        "    Args:\n",
        "        See original evaluate_ir_system function\n",
        "    \n",
        "    Returns:\n",
        "        Evaluation metrics\n",
        "    \"\"\"\n",
        "    metrics_per_query = []\n",
        "    all_relevant_doc_ids = []\n",
        "    all_retrieved_doc_ids = []\n",
        "\n",
        "    for i, row in queries.iterrows():\n",
        "        query_text = row['q']\n",
        "        relevant_doc_ids = row['r']\n",
        "        relevant_doc_ids = [doc[0] for doc in relevant_doc_ids]\n",
        "        \n",
        "        # First get top k results\n",
        "        results = retrieve_documents(query_text, recipies, recipe_ids, k=int(k), threshold=None)\n",
        "        \n",
        "        # Then filter by threshold\n",
        "        filtered_results = [r for r in results if r[2] >= threshold]\n",
        "        retrieved_doc_ids = [result[1] for result in filtered_results]\n",
        "\n",
        "        query_metrics = calculate_precision_recall_f1_optimized(\n",
        "            relevant_doc_ids, retrieved_doc_ids\n",
        "        )\n",
        "        metrics_per_query.append(query_metrics)\n",
        "\n",
        "        all_relevant_doc_ids.append(relevant_doc_ids)\n",
        "        all_retrieved_doc_ids.append(retrieved_doc_ids)\n",
        "\n",
        "    macro_metrics = calculate_macro_averages(metrics_per_query)\n",
        "    micro_metrics = calculate_micro_averages_optimized(\n",
        "        all_relevant_doc_ids, all_retrieved_doc_ids\n",
        "    )\n",
        "\n",
        "    all_metrics = {**macro_metrics, **micro_metrics}\n",
        "    return all_metrics\n",
        "\n",
        "# Main execution\n",
        "# Run parameter tuning\n",
        "results = find_optimal_parameters(queries, recipies, recipe_ids)\n",
        "\n",
        "# Plot results\n",
        "plot_parameter_results(results['results_df'])\n",
        "\n",
        "# Final evaluation with best parameters\n",
        "if 'threshold' in results['best_parameter']:\n",
        "    print(\"\\n\\nFinal evaluation with best threshold:\")\n",
        "    final_metrics = evaluate_ir_system(queries, recipies, recipe_ids, \n",
        "                                        k=None, threshold=results['best_threshold'])\n",
        "else:\n",
        "    print(\"\\n\\nFinal evaluation with best k:\")\n",
        "    final_metrics = evaluate_ir_system(queries, recipies, recipe_ids, \n",
        "                                        k=results['best_k'], threshold=None)\n",
        "\n",
        "# Print the final results\n",
        "print(\"\\n===== IR System Evaluation Results with Optimal Parameters =====\")\n",
        "print(f\"Optimal parameters: {results['best_parameter']}\")\n",
        "print(f\"Macro-average Precision: {final_metrics['macro_precision']:.4f}\")\n",
        "print(f\"Macro-average Recall: {final_metrics['macro_recall']:.4f}\")\n",
        "print(f\"Macro-average F1: {final_metrics['macro_f1']:.4f}\")\n",
        "print(f\"Micro-average Precision: {final_metrics['micro_precision']:.4f}\")\n",
        "print(f\"Micro-average Recall: {final_metrics['micro_recall']:.4f}\")\n",
        "print(f\"Micro-average F1: {final_metrics['micro_f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aUQ2w_FV4ML"
      },
      "source": [
        "For a given query and set of relevant documents, you are also required to create a prompt that instructs a model to complete a certain task (e.g. recipe recommendation). You should experiment with formatting the prompt, as language models have been shown to be sensitive to the exact verbiage of instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMH0BuyxkVrb"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "# Recipe Assistant\n",
        "\n",
        "## Context\n",
        "You are a helpful recipe assistant with access to a database of recipes. The system has already retrieved the most relevant recipes to the user's query using TF-IDF similarity. Your goal is to provide helpful, accurate responses about recipes, cooking techniques, ingredient substitutions, and culinary advice based on the retrieved recipes.\n",
        "\n",
        "## Retrieved Recipes\n",
        "The following recipes have been retrieved as most relevant to the user's query:\n",
        "\n",
        "{retrieved_recipes}\n",
        "\n",
        "## Instructions\n",
        "1. **Answer directly from the retrieved recipes when possible.** Use the information from the provided recipes to answer questions about ingredients, cooking methods, nutritional information, and preparation steps.\n",
        "\n",
        "2. **For ingredient questions:**\n",
        "   - Provide accurate amounts and measurements from the recipes\n",
        "   - Suggest possible substitutions based on common culinary knowledge\n",
        "   - Explain the purpose of key ingredients in the dish\n",
        "\n",
        "3. **For cooking technique questions:**\n",
        "   - Explain preparation methods mentioned in the recipes\n",
        "   - Clarify cooking times and temperatures\n",
        "   - Describe expected results and how to tell when food is properly cooked\n",
        "\n",
        "4. **For modification requests:**\n",
        "   - Suggest appropriate adjustments for dietary restrictions (vegan, gluten-free, etc.)\n",
        "   - Explain how to scale recipes up or down\n",
        "   - Offer ideas for flavor variations while maintaining the core identity of the dish\n",
        "\n",
        "5. **For general questions:**\n",
        "   - Provide brief culinary background/history when relevant\n",
        "   - Explain unfamiliar cooking terms\n",
        "   - Suggest pairings, serving suggestions, and storage recommendations\n",
        "\n",
        "## Response Format\n",
        "- Start with a direct answer to the user's question\n",
        "- Keep your responses concise but comprehensive\n",
        "- For multi-step instructions or complex concepts, organize information in a clear, logical structure\n",
        "- If the retrieved recipes don't contain sufficient information to answer the query, acknowledge the limitations and provide general culinary knowledge that might help\n",
        "- When suggesting modifications not explicitly in the retrieved recipes, clearly indicate these are your recommendations based on culinary principles\n",
        "\n",
        "## Limitations\n",
        "- Don't make claims about specific nutritional values unless they're mentioned in the retrieved recipes\n",
        "- If asked about topics completely unrelated to cooking or the recipes provided, politely redirect the conversation back to recipe-related topics\n",
        "- Don't invent or fabricate details about recipes that aren't in the retrieved data\n",
        "\n",
        "## User Query\n",
        "{user_query}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCxrQbNjXJze"
      },
      "outputs": [],
      "source": [
        "irrelevant_context = \"\"\"\n",
        "Richard Gary Brautigan (January 30, 1935 – c. September 16, 1984)\n",
        "was an American novelist, poet, and short story writer. A prolific writer,\n",
        "he wrote throughout his life and published ten novels, two collections of\n",
        "short stories, and four books of poetry. Brautigan's work has been published\n",
        "both in the United States and internationally throughout Europe, Japan,\n",
        "and China. He is best known for his novels Trout Fishing in America (1967),\n",
        "In Watermelon Sugar (1968), and The Abortion: An Historical Romance 1966 (1971).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf67P4TgVvd1"
      },
      "source": [
        "**IMPORTANT**: only run the following code when you have implemented a working retrieval system. When you are ready to work with language models, navigate to the menu bar in Colab and select **Runtime > Change runtime type > T4 GPU**. If you find yourself working on not GPU-intenstive tasks in this notebook, change your runtime back to CPU to preserve access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPBnqJFRbS2G"
      },
      "outputs": [],
      "source": [
        "! pip -q install git+https://github.com/huggingface/transformers\n",
        "! pip -q install datasets bitsandbytes accelerate xformers einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9QXH6gVbtuC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W59_uVOuIPJ6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace \"YOUR_HF_TOKEN\" with your actual Hugging Face token\n",
        "login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyMVZEHXYbRz"
      },
      "source": [
        "The code below will load a Mistral 7B instruct model and quantize it via `bitesandbytes`. Doing so will ensure that the model will not take up too much memory and make inference more efficient. Note that the call to `AutoModelForCausalLM.from_pretrained()` will take a while, as the model's weights must be downloaded from the huggingface hub. Also note that you are not restricted to using Mistral, and are welcome to experiment with other models (though you will have more luck with chat and instruction-tuned variants)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEuL6xNM-TD2"
      },
      "outputs": [],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTeEJDcr-ZSL"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEJJsLzQdK9q"
      },
      "source": [
        "A tokenizer is required in order to convert strings into integer sequences that can be passed as input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxgIwhXU-gyS"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-2ofJeBdgJF"
      },
      "outputs": [],
      "source": [
        "retrieved_recipes = \"1. Chocolate Chip Cookies...\\n2. Brownie Bites...\"\n",
        "user_query = \"Can I use coconut oil instead of butter in cookies?\"\n",
        "\n",
        "# Fill in the template\n",
        "input_string_with_context = prompt.format(\n",
        "    retrieved_recipes=retrieved_recipes, user_query=user_query\n",
        ")\n",
        "\n",
        "input_string_without_context = prompt.format(\n",
        "    retrieved_recipes=irrelevant_context, user_query=user_query\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtQNQGwj-nbM"
      },
      "outputs": [],
      "source": [
        "encoded_prompt = tokenizer(\n",
        "    input_string_with_context, return_tensors=\"pt\", add_special_tokens=False\n",
        ")\n",
        "encoded_prompt = encoded_prompt.to(\"cuda\")\n",
        "generated_ids = model.generate(**encoded_prompt, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfjgBnG--te6"
      },
      "outputs": [],
      "source": [
        "encoded_prompt = tokenizer(\n",
        "    input_string_without_context, return_tensors=\"pt\", add_special_tokens=False\n",
        ")\n",
        "encoded_prompt = encoded_prompt.to(\"cuda\")\n",
        "generated_ids = model.generate(**encoded_prompt, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTKh_sKlLjBp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
