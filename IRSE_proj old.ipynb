{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5hH-WBI1R2H"
      },
      "source": [
        "# H02C8b Information Retrieval and Search Engines: RAG Project\n",
        "\n",
        "Welcome to the notebook companion for the IRSE project. You will find all starter code here. You are encouraged to use this code, as it has been confirmed to work for the RAG pipeline described in the assignment handout. However, you are certainly welcome to make any changes you see fit, provided that your code is written in Python and runs without issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kkz_9Y7FrLe"
      },
      "source": [
        "**IMPORTANT**: Do not submit a notebook as your final solution. It will not be graded. Refer to assignment handout for more information about the submission format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw2_H111Frbu"
      },
      "source": [
        "**IMPORTANT**: Be mindful of your runtime usage, if working in Colab. At the beginning of every session, navigate to the top menu bar in Colab and select **Runtime > Change runtime type > CPU (Python 3)**. This will ensure that your session runs on CPU and that you do not waste any GPU allocation for the day. GPUs are provided by Google on a limited daily basis, and access is given every 24 hours. It is best that you complete the TF-IDF/search component before loading models and running inference on the GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM9AFxoyfQZT"
      },
      "source": [
        "If you have any questions, feel free to email [Thomas](mailto:thomas.bauwens@kuleuven.be) or [Kushal](mailto:kushaljayesh.tatariya@kuleuven.be)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpcHebZfGi-f"
      },
      "source": [
        "## RAG for recipe recommendation:\n",
        "\n",
        "We will begin by installing the huggingface `datasets` library for easily loading our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oy3FsTE2bKfU"
      },
      "outputs": [],
      "source": [
        "# ! pip -q install datasets\n",
        "# !wget https://people.cs.kuleuven.be/~thomas.bauwens/irse_documents_2025_recipes.parquet\n",
        "# !wget https://people.cs.kuleuven.be/~thomas.bauwens/irse_queries_2025_recipes.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIq7VwBoFvV1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/yan/yan_files/2_semester/information_retrieval/project/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to /home/yan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/yan/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/yan/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/yan/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /home/yan/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /home/yan/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/yan/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import datasets\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import datasets\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()  # Show progress bar if using pandas\n",
        "\n",
        "import nltk\n",
        "from utils import *\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# from google.colab import userdata\n",
        "# userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "08BidyMMGSpB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 231637\n",
            "Number of queries: 47\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"parquet\", data_files=\"./irse_documents_2025_recipes.parquet\"\n",
        ")[\"train\"]\n",
        "queries_data = json.load(open(\"./irse_queries_2025_recipes.json\", \"r\"))\n",
        "\n",
        "df = dataset.to_pandas()\n",
        "\n",
        "# Now you can apply the function to concatenate columns\n",
        "recipies = df.apply(\n",
        "    lambda row: f\"{row['name']} {row['description']} {row['ingredients']} {row['steps']}\",\n",
        "    axis=1,\n",
        ")  # [:10000]\n",
        "recipe_ids = dataset[\"official_id\"]  # [:10000]\n",
        "print(\"Number of documents:\", len(recipies))\n",
        "\n",
        "queries = pd.DataFrame(columns=[\"q\", \"r\", \"a\"])\n",
        "for query_item in queries_data[\"queries\"]:\n",
        "    query_text = query_item[\"q\"]\n",
        "    relevance_pairs = query_item[\"r\"]\n",
        "    answer = query_item[\"a\"]\n",
        "    queries = pd.concat(\n",
        "        [\n",
        "            queries,\n",
        "            pd.DataFrame({\"q\": [query_text], \"r\": [relevance_pairs], \"a\": [answer]}),\n",
        "        ],\n",
        "        ignore_index=True,\n",
        "    )\n",
        "\n",
        "print(\"Number of queries:\", len(queries))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def preprocess_vocabulary(doc):\n",
        "    doc = doc.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n",
        "\n",
        "    words = word_tokenize(doc)\n",
        "\n",
        "    words = [\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in words\n",
        "        if word not in stop_words \n",
        "    ]\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "preprocessed_recipes = [preprocess_vocabulary(doc) for doc in tqdm(recipies)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PEQStiG7xtC"
      },
      "source": [
        "#### Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EAEZMxZGus6M"
      },
      "outputs": [],
      "source": [
        "# lemmatizer = WordNetLemmatizer()\n",
        "# counter = 0\n",
        "\n",
        "\n",
        "# def preprocess(doc):\n",
        "#     global counter\n",
        "#     counter += 1\n",
        "#     if counter % 1000 == 0:\n",
        "#         print(f\"Processed: {counter}\")\n",
        "#     doc = doc.split()\n",
        "#     preprocessed_text = []\n",
        "#     for text in doc:\n",
        "#         text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "#         text = text.lower()\n",
        "#         words = word_tokenize(text)\n",
        "#         words = [word for word in words if word not in stopwords.words(\"english\")]\n",
        "#         words = [lemmatizer.lemmatize(word) for word in words]\n",
        "#         if words != []:\n",
        "#             preprocessed_text.append(words[0])\n",
        "#     return preprocessed_text\n",
        "# vectorizer = TfidfVectorizer(tokenizer=preprocess)\n",
        "# X = vectorizer.fit_transform(recipies)\n",
        "# print(\"fitted\")\n",
        "# tfidf_df = pd.DataFrame(\n",
        "#     X.toarray(), index=range(len(recipies)), columns=vectorizer.get_feature_names_out()\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF fitted\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=5000,  # Top 5000 most frequent terms\n",
        "    # min_df=5,               # Ignore terms that appear in fewer than 5 documents\n",
        "    max_df=0.8,  # Ignore terms that appear in more than 80% of docs\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(preprocessed_recipes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O9GR23l7xtC"
      },
      "source": [
        "#### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_documents(query_text, recipies, recipe_ids, k=None, threshold=None):\n",
        "    if len(recipies) != len(recipe_ids):\n",
        "        raise ValueError(\"Recipes and recipe_ids must have the same length\")\n",
        "    if k is None and threshold is None:\n",
        "        raise ValueError(\"Either k or threshold must be specified\")\n",
        "    if vectorizer is None or X is None:\n",
        "        raise ValueError(\"Vectorizer and document matrix X must be provided\")\n",
        "\n",
        "    query = preprocess_vocabulary(query_text)\n",
        "    query_vector = vectorizer.transform([query])\n",
        "    cosine_similarities = cosine_similarity(query_vector, X).flatten()\n",
        "\n",
        "    results = [\n",
        "        (recipies[i], recipe_ids[i], cosine_similarities[i])\n",
        "        for i in range(len(recipies))\n",
        "    ]\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "    if threshold is not None:\n",
        "        results = [r for r in results if r[2] >= threshold]\n",
        "\n",
        "    if k is not None:\n",
        "        results = results[:k]\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_ir_system(queries, recipies, recipe_ids, k=None, threshold=None):\n",
        "    metrics_per_query = []\n",
        "    all_relevant_doc_ids = []\n",
        "    all_retrieved_doc_ids = []\n",
        "\n",
        "    for _, row in queries.iterrows():\n",
        "        query_text = row[\"q\"]\n",
        "        relevant_doc_ids = [doc[0] for doc in row[\"r\"]]\n",
        "\n",
        "        results = retrieve_documents(query_text, recipies, recipe_ids, k, threshold)\n",
        "        retrieved_doc_ids = [result[1] for result in results]\n",
        "\n",
        "        query_metrics = calculate_precision_recall_f1_optimized(\n",
        "            relevant_doc_ids, retrieved_doc_ids\n",
        "        )\n",
        "        metrics_per_query.append(query_metrics)\n",
        "\n",
        "        all_relevant_doc_ids.append(relevant_doc_ids)\n",
        "        all_retrieved_doc_ids.append(retrieved_doc_ids)\n",
        "\n",
        "    macro_metrics = calculate_macro_averages(metrics_per_query)\n",
        "    micro_metrics = calculate_micro_averages_optimized(\n",
        "        all_relevant_doc_ids, all_retrieved_doc_ids\n",
        "    )\n",
        "\n",
        "    all_metrics = {**macro_metrics, **micro_metrics}\n",
        "    return all_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_K=29\n",
        "best_threshold=0.24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'macro_precision': np.float64(0.9375),\n",
              " 'macro_recall': np.float64(0.09933774834437085),\n",
              " 'macro_f1': np.float64(0.17964071856287422),\n",
              " 'micro_precision': 0.9375,\n",
              " 'micro_recall': 0.09933774834437085,\n",
              " 'micro_f1': 0.17964071856287422}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_ir_system(queries.iloc[[34]], recipies, recipe_ids, k=best_K, threshold=best_threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== IR System Evaluation Results =====\n",
            "Macro-average Precision: 0.2681\n",
            "Macro-average Recall: 0.3284\n",
            "Macro-average F1: 0.1838\n",
            "Micro-average Precision: 0.3122\n",
            "Micro-average Recall: 0.1209\n",
            "Micro-average F1: 0.1743\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "metrics = evaluate_ir_system(queries, recipies, recipe_ids, k=best_K, threshold=best_threshold)\n",
        "\n",
        "print(\"\\n===== IR System Evaluation Results =====\")\n",
        "print(f\"Macro-average Precision: {metrics['macro_precision']:.4f}\")\n",
        "print(f\"Macro-average Recall: {metrics['macro_recall']:.4f}\")\n",
        "print(f\"Macro-average F1: {metrics['macro_f1']:.4f}\")\n",
        "print(f\"Micro-average Precision: {metrics['micro_precision']:.4f}\")\n",
        "print(f\"Micro-average Recall: {metrics['micro_recall']:.4f}\")\n",
        "print(f\"Micro-average F1: {metrics['micro_f1']:.4f}\")\n",
        "print(\"========================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aUQ2w_FV4ML"
      },
      "source": [
        "For a given query and set of relevant documents, you are also required to create a prompt that instructs a model to complete a certain task (e.g. recipe recommendation). You should experiment with formatting the prompt, as language models have been shown to be sensitive to the exact verbiage of instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'create_parameter_heatmap' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_parameter_heatmap\u001b[49m(queries, recipies, recipe_ids)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'create_parameter_heatmap' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "results = create_parameter_heatmap(queries, recipies, recipe_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tMH0BuyxkVrb"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "\n",
        "# Recipe Assistant\n",
        "\n",
        "## Context\n",
        "You are a helpful recipe assistant with access to a database of recipes. The system has already retrieved the most relevant recipes to the user's query using TF-IDF similarity. Your goal is to provide helpful, accurate responses about recipes, cooking techniques, ingredient substitutions, and culinary advice based on the retrieved recipes.\n",
        "\n",
        "## Retrieved Recipes\n",
        "The following recipes have been retrieved as most relevant to the user's query:\n",
        "\n",
        "{retrieved_recipes}\n",
        "\n",
        "## Instructions\n",
        "1. **Answer directly from the retrieved recipes when possible.** Use the information from the provided recipes to answer questions about ingredients, cooking methods, nutritional information, and preparation steps.\n",
        "\n",
        "2. **For ingredient questions:**\n",
        "   - Provide accurate amounts and measurements from the recipes\n",
        "   - Suggest possible substitutions based on common culinary knowledge\n",
        "   - Explain the purpose of key ingredients in the dish\n",
        "\n",
        "3. **For cooking technique questions:**\n",
        "   - Explain preparation methods mentioned in the recipes\n",
        "   - Clarify cooking times and temperatures\n",
        "   - Describe expected results and how to tell when food is properly cooked\n",
        "\n",
        "4. **For modification requests:**\n",
        "   - Suggest appropriate adjustments for dietary restrictions (vegan, gluten-free, etc.)\n",
        "   - Explain how to scale recipes up or down\n",
        "   - Offer ideas for flavor variations while maintaining the core identity of the dish\n",
        "\n",
        "5. **For general questions:**\n",
        "   - Provide brief culinary background/history when relevant\n",
        "   - Explain unfamiliar cooking terms\n",
        "   - Suggest pairings, serving suggestions, and storage recommendations\n",
        "\n",
        "## Response Format\n",
        "- Start with a direct answer to the user's question\n",
        "- Keep your responses concise but comprehensive\n",
        "- For multi-step instructions or complex concepts, organize information in a clear, logical structure\n",
        "- If the retrieved recipes don't contain sufficient information to answer the query, acknowledge the limitations and provide general culinary knowledge that might help\n",
        "- When suggesting modifications not explicitly in the retrieved recipes, clearly indicate these are your recommendations based on culinary principles\n",
        "\n",
        "## Limitations\n",
        "- Don't make claims about specific nutritional values unless they're mentioned in the retrieved recipes\n",
        "- If asked about topics completely unrelated to cooking or the recipes provided, politely redirect the conversation back to recipe-related topics\n",
        "- Don't invent or fabricate details about recipes that aren't in the retrieved data\n",
        "\n",
        "## User Query\n",
        "{user_query}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "VCxrQbNjXJze"
      },
      "outputs": [],
      "source": [
        "irrelevant_context = \"\"\"\n",
        "Richard Gary Brautigan (January 30, 1935 – c. September 16, 1984)\n",
        "was an American novelist, poet, and short story writer. A prolific writer,\n",
        "he wrote throughout his life and published ten novels, two collections of\n",
        "short stories, and four books of poetry. Brautigan's work has been published\n",
        "both in the United States and internationally throughout Europe, Japan,\n",
        "and China. He is best known for his novels Trout Fishing in America (1967),\n",
        "In Watermelon Sugar (1968), and The Abortion: An Historical Romance 1966 (1971).\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf67P4TgVvd1"
      },
      "source": [
        "**IMPORTANT**: only run the following code when you have implemented a working retrieval system. When you are ready to work with language models, navigate to the menu bar in Colab and select **Runtime > Change runtime type > T4 GPU**. If you find yourself working on not GPU-intenstive tasks in this notebook, change your runtime back to CPU to preserve access.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lPBnqJFRbS2G"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires scipy, which is not installed.\n",
            "sentence-transformers 3.4.1 requires torch>=1.11.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip -q install git+https://github.com/huggingface/transformers\n",
        "! pip -q install datasets bitsandbytes accelerate xformers einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9QXH6gVbtuC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from google.colab import userdata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W59_uVOuIPJ6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Replace \"YOUR_HF_TOKEN\" with your actual Hugging Face token\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyMVZEHXYbRz"
      },
      "source": [
        "The code below will load a Mistral 7B instruct model and quantize it via `bitesandbytes`. Doing so will ensure that the model will not take up too much memory and make inference more efficient. Note that the call to `AutoModelForCausalLM.from_pretrained()` will take a while, as the model's weights must be downloaded from the huggingface hub. Also note that you are not restricted to using Mistral, and are welcome to experiment with other models (though you will have more luck with chat and instruction-tuned variants)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEuL6xNM-TD2"
      },
      "outputs": [],
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTeEJDcr-ZSL"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id, trust_remote_code=True, quantization_config=bnb_config, device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEJJsLzQdK9q"
      },
      "source": [
        "A tokenizer is required in order to convert strings into integer sequences that can be passed as input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxgIwhXU-gyS"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-2ofJeBdgJF"
      },
      "outputs": [],
      "source": [
        "retrieved_recipes = \"1. Chocolate Chip Cookies...\\n2. Brownie Bites...\"\n",
        "user_query = \"Can I use coconut oil instead of butter in cookies?\"\n",
        "\n",
        "# Fill in the template\n",
        "input_string_with_context = prompt.format(\n",
        "    retrieved_recipes=retrieved_recipes, user_query=user_query\n",
        ")\n",
        "\n",
        "input_string_without_context = prompt.format(\n",
        "    retrieved_recipes=irrelevant_context, user_query=user_query\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtQNQGwj-nbM"
      },
      "outputs": [],
      "source": [
        "encoded_prompt = tokenizer(\n",
        "    input_string_with_context, return_tensors=\"pt\", add_special_tokens=False\n",
        ")\n",
        "encoded_prompt = encoded_prompt.to(\"cuda\")\n",
        "generated_ids = model.generate(**encoded_prompt, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfjgBnG--te6"
      },
      "outputs": [],
      "source": [
        "encoded_prompt = tokenizer(\n",
        "    input_string_without_context, return_tensors=\"pt\", add_special_tokens=False\n",
        ")\n",
        "encoded_prompt = encoded_prompt.to(\"cuda\")\n",
        "generated_ids = model.generate(**encoded_prompt, max_new_tokens=1000, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTKh_sKlLjBp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
